{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ΕΡΓΑΣΙΑ ΓΙΑ ΤΟ ΜΑΘΗΜΑ KNOWLEDGE REPRESENTATION AND BIG DATA (ΜΕΡΟΣ B’)\n",
    "## Παππάς Παντελεήμων mscaidl-0030@uniwa.gr\n",
    "\n",
    "This notebook aims to demonstrate use of Spark in order to deal with big data in a DDoS analysis scenario and actually using the ML libraries of Spark.\n",
    "\n",
    "It is set up to run in an Anaconda enviroment as the installation is fairly painless and straight forward.\n",
    "\n",
    "If you wish to run this notebook I recommend Anaconda paired with VSCode, as VSCode will resolve dependencies automatically.\n",
    "I also recommend using a clean conda environment for Spark installation. \n",
    "\n",
    "I'll include two helpful links just in case: \n",
    "* https://medium.com/@divya.chandana/easy-install-pyspark-in-anaconda-e2d427b3492f\n",
    "* https://www.youtube.com/watch?v=KCo59BnKxkk\n",
    "\n",
    "If there are any problems, please contact me. Otherwise you should be able to see my saved results in the notebook."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import sys\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import mean,col,split, col, regexp_extract, when, lit\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.feature import QuantileDiscretizer\n",
    "from pyspark.sql.functions import countDistinct\n",
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "def groupby_apply_describe(df, groupby_col, stat_col):\n",
    "    \"\"\"From a grouby df object provide the stats\n",
    "    of describe for each key in the groupby object.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : spark dataframe groupby object\n",
    "    col : column to compute statistics on\n",
    "    \n",
    "    \"\"\"\n",
    "    output = df.groupby(groupby_col).agg(\n",
    "        F.count(stat_col).alias(\"count\"),\n",
    "        F.mean(stat_col).alias(\"mean\"),\n",
    "        F.stddev(stat_col).alias(\"std\"),\n",
    "        F.min(stat_col).alias(\"min\"),\n",
    "        F.max(stat_col).alias(\"max\"),\n",
    "    )\n",
    "    print(output.orderBy(groupby_col).show())\n",
    "    return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Spark Session and Spark context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create SparkSession and sparkcontext\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder\\\n",
    "                    .master(\"local\")\\\n",
    "                    .appName('Pantelis Pappas Spark Project')\\\n",
    "                    .getOrCreate()\n",
    "sc=spark.sparkContext\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Suppressing log warnings\n",
    "sc.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kill all spark session and spark context (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopping Spark-Session and Spark context\n",
    "sc.stop()\n",
    "spark.stop()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple word counting program (Υλοποίηση του Μέρους Α σε Spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the text file\n",
    "input_file = sc.textFile(\"hadoopPPtext.txt\")\n",
    "#Mapping\n",
    "map = input_file.flatMap(lambda line: line.split(\" \")).map(lambda word:(word, 1))\n",
    "#Reducing\n",
    "counts = map.reduceByKey(lambda a, b: a + b)\n",
    "# counts.saveAsTextFile(\"/home/dante/SparkProject/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====: 5\n",
      "Transaction: 3\n",
      "Inputs: 1\n",
      ": 86\n",
      "(((\"transactions\",: 1\n",
      "\"outputs: 2\n",
      "and: 21\n",
      "inputs\",: 4\n",
      "\"input: 4\n",
      "components\")))(((\"outputs: 1\n",
      "components\")))(((\"unspent: 1\n",
      "transaction: 24\n",
      "outputs: 2\n",
      "(UTXO)\")))(((\"UTXO: 1\n",
      "sets\")))Transaction: 1\n",
      "inputs: 11\n",
      "identify: 3\n",
      "(by: 1\n",
      "reference): 1\n",
      "which: 8\n",
      "UTXO: 26\n",
      "will: 6\n",
      "be: 7\n",
      "consumed: 2\n",
      "provide: 1\n",
      "proof: 1\n",
      "of: 18\n",
      "ownership: 2\n",
      "through: 1\n",
      "an: 8\n",
      "unlocking: 10\n",
      "script.: 2\n",
      "To: 3\n",
      "build: 3\n",
      "a: 18\n",
      "transaction,: 4\n",
      "wallet: 6\n",
      "selects: 1\n",
      "from: 9\n",
      "the: 73\n",
      "it: 7\n",
      "controls,: 1\n",
      "with: 7\n",
      "enough: 1\n",
      "value: 6\n",
      "to: 29\n",
      "make: 3\n",
      "requested: 1\n",
      "payment.: 1\n",
      "Sometimes: 1\n",
      "one: 7\n",
      "is: 22\n",
      "enough,: 1\n",
      "other: 2\n",
      "times: 1\n",
      "more: 1\n",
      "than: 2\n",
      "needed.: 1\n",
      "For: 2\n",
      "each: 1\n",
      "that: 20\n",
      "this: 7\n",
      "payment,: 1\n",
      "creates: 1\n",
      "input: 12\n",
      "pointing: 1\n",
      "unlocks: 1\n",
      "Let's: 1\n",
      "look: 1\n",
      "at: 2\n",
      "components: 1\n",
      "in: 36\n",
      "greater: 1\n",
      "detail.: 1\n",
      "The: 14\n",
      "first: 5\n",
      "part: 3\n",
      "pointer: 1\n",
      "by: 6\n",
      "reference: 3\n",
      "hash: 1\n",
      "output: 4\n",
      "index,: 1\n",
      "identifies: 1\n",
      "specific: 2\n",
      "transaction.: 2\n",
      "second: 1\n",
      "script,: 3\n",
      "constructs: 1\n",
      "order: 4\n",
      "satisfy: 2\n",
      "spending: 3\n",
      "conditions: 4\n",
      "set: 2\n",
      "UTXO.: 1\n",
      "Most: 1\n",
      "often,: 1\n",
      "script: 8\n",
      "digital: 1\n",
      "signature: 4\n",
      "public: 2\n",
      "key: 2\n",
      "proving: 1\n",
      "bitcoin.: 1\n",
      "However,: 1\n",
      "not: 5\n",
      "all: 1\n",
      "scripts: 1\n",
      "contain: 1\n",
      "signatures.: 1\n",
      "third: 1\n",
      "sequence: 4\n",
      "number,: 1\n",
      "discussed: 2\n",
      "later.: 1\n",
      "Consider: 1\n",
      "our: 1\n",
      "example: 1\n",
      "<<transactions_behind_the_scenes>>.: 1\n",
      "are: 4\n",
      "array: 1\n",
      "(list): 1\n",
      "called: 1\n",
      "+vin+:: 1\n",
      "[[vin]]: 1\n",
      ".The: 1\n",
      "Alice's: 9\n",
      "[source,json]: 3\n",
      "----: 8\n",
      "\"vin\":: 2\n",
      "[: 3\n",
      "{: 3\n",
      "\"txid\":: 2\n",
      "\"7957a35fe64f80d234d76d83a2a8f1a0d8149a41d81de548f0a65a8a999f6f18\",: 2\n",
      "\"vout\":: 3\n",
      "0,: 2\n",
      "\"scriptSig\": 2\n",
      ":: 2\n",
      "\"3045022100884d142d86652a3f47ba4746ec719bbfbd040a570b1deccbb6498c75c4ae24cb02204b9f039ff08df09cbe9f6addac960298cad530a863ea8f53982c09db8f6e3813[ALL]: 2\n",
      "0484ecc0d46f1918b30928fa0e4ed99f16a0fb4fde0735e7ade8416ab9fe423cc5412336376789d172787ec3457eee41c04f4938de5cc17b4a10fa336a8d752adf\",: 2\n",
      "\"sequence\":: 2\n",
      "4294967295: 2\n",
      "}: 3\n",
      "]: 2\n",
      "As: 2\n",
      "you: 5\n",
      "can: 5\n",
      "see,: 1\n",
      "there: 1\n",
      "only: 1\n",
      "list: 1\n",
      "(because: 1\n",
      "contained: 1\n",
      "sufficient: 1\n",
      "payment).: 1\n",
      "contains: 4\n",
      "four: 1\n",
      "elements:: 1\n",
      "*: 8\n",
      "A: 6\n",
      "(((\"transaction: 1\n",
      "IDs: 1\n",
      "(txd)\")))transaction: 1\n",
      "ID,: 1\n",
      "referencing: 1\n",
      "being: 1\n",
      "spent: 2\n",
      "An: 1\n",
      "index: 4\n",
      "(+vout+),: 1\n",
      "identifying: 1\n",
      "referenced: 11\n",
      "(first: 1\n",
      "zero): 1\n",
      "+scriptSig+,: 1\n",
      "satisfies: 1\n",
      "placed: 1\n",
      "on: 4\n",
      "UTXO,: 3\n",
      "for: 5\n",
      "number: 3\n",
      "(to: 1\n",
      "later): 1\n",
      "In: 1\n",
      "points: 1\n",
      "ID:: 1\n",
      "7957a35fe64f80d234d76d83a2a8f1a0d8149a41d81de548f0a65a8a999f6f18: 1\n",
      "+0+: 1\n",
      "(i.e.,: 1\n",
      "created: 1\n",
      "transaction).: 1\n",
      "constructed: 1\n",
      "retrieving: 5\n",
      "examining: 1\n",
      "its: 2\n",
      "locking: 5\n",
      "then: 1\n",
      "using: 1\n",
      "necessary: 1\n",
      "it.: 4\n",
      "Looking: 1\n",
      "just: 2\n",
      "may: 1\n",
      "have: 2\n",
      "noticed: 1\n",
      "we: 10\n",
      "don't: 3\n",
      "know: 6\n",
      "anything: 1\n",
      "about: 1\n",
      "parent: 2\n",
      "containing: 2\n",
      "We: 3\n",
      "(amount: 1\n",
      "satoshi),: 1\n",
      "sets: 1\n",
      "find: 2\n",
      "information,: 1\n",
      "must: 3\n",
      "retrieve: 5\n",
      "Notice: 1\n",
      "because: 2\n",
      "explicitly: 1\n",
      "stated,: 1\n",
      "also: 3\n",
      "use: 2\n",
      "calculate: 2\n",
      "fees: 3\n",
      "paid: 2\n",
      "(see: 1\n",
      "<<tx_fees>>).: 1\n",
      "It's: 1\n",
      "needs: 1\n",
      "inputs.: 2\n",
      "Once: 1\n",
      "broadcast: 1\n",
      "network,: 3\n",
      "every: 2\n",
      "validating: 2\n",
      "node: 1\n",
      "need: 1\n",
      "validate: 1\n",
      "Transactions: 1\n",
      "their: 5\n",
      "own: 1\n",
      "seem: 1\n",
      "incomplete: 1\n",
      "they: 1\n",
      "lack: 1\n",
      "context.: 1\n",
      "They: 1\n",
      "but: 2\n",
      "without: 2\n",
      "cannot: 1\n",
      "or: 6\n",
      "conditions.: 1\n",
      "When: 1\n",
      "writing: 1\n",
      "bitcoin: 2\n",
      "software,: 1\n",
      "anytime: 1\n",
      "decode: 1\n",
      "intent: 1\n",
      "counting: 2\n",
      "checking: 1\n",
      "your: 1\n",
      "code: 1\n",
      "blockchain: 1\n",
      "context: 1\n",
      "implied: 1\n",
      "present: 1\n",
      "references: 1\n",
      "example,: 1\n",
      "amount: 1\n",
      "fees,: 1\n",
      "sum: 1\n",
      "values: 1\n",
      "outputs.: 1\n",
      "But: 1\n",
      "inputs,: 1\n",
      "do: 1\n",
      "value.: 1\n",
      "So: 1\n",
      "seemingly: 1\n",
      "simple: 1\n",
      "operation: 1\n",
      "like: 1\n",
      "single: 1\n",
      "fact: 1\n",
      "involves: 1\n",
      "multiple: 2\n",
      "steps: 1\n",
      "data: 1\n",
      "transactions.: 1\n",
      "same: 1\n",
      "commands: 1\n",
      "Bitcoin: 1\n",
      "Core: 1\n",
      "as: 4\n",
      "used: 1\n",
      "when: 2\n",
      "(+getrawtransaction+: 1\n",
      "+decoderawtransaction+).: 1\n",
      "With: 1\n",
      "get: 1\n",
      "take: 1\n",
      "look:: 1\n",
      "[[alice_input_tx]]: 1\n",
      ".UTXO: 1\n",
      "previous: 3\n",
      "\"value\":: 1\n",
      "0.10000000,: 1\n",
      "\"scriptPubKey\":: 1\n",
      "\"OP_DUP: 2\n",
      "OP_HASH160: 1\n",
      "7f9b1a7fb68d60c536c2fd8aeaa53a8f3cc025a8: 1\n",
      "OP_EQUALVERIFY: 1\n",
      "OP_CHECKSIG\": 1\n",
      "see: 3\n",
      "has: 2\n",
      "0.1: 1\n",
      "BTC: 1\n",
      "(+scriptPubKey+): 1\n",
      "OP_HASH160...\".: 1\n",
      "[TIP]: 1\n",
      "fully: 1\n",
      "understand: 1\n",
      "had: 1\n",
      "input.: 1\n",
      "function: 1\n",
      "retrieves: 1\n",
      "transactions: 2\n",
      "unspent: 1\n",
      "very: 1\n",
      "common: 1\n",
      "exists: 1\n",
      "almost: 1\n",
      "library: 1\n",
      "API.: 1\n",
      "=====: 1\n",
      "serialization&#x2014;inputs: 1\n",
      "(((\"serialization\",: 1\n",
      "\"inputs\")))(((\"transactions\",: 1\n",
      "serialization\")))(((\"outputs: 1\n",
      "serialization\")))When: 1\n",
      "serialized: 6\n",
      "transmission: 2\n",
      "encoded: 2\n",
      "into: 2\n",
      "byte: 3\n",
      "stream: 2\n",
      "shown: 2\n",
      "<<tx_in_structure>>.: 1\n",
      "[[tx_in_structure]]: 1\n",
      ".Transaction: 1\n",
      "serialization: 3\n",
      "[options=\"header\"]: 2\n",
      "|=======: 4\n",
      "|Size|: 2\n",
      "Field: 2\n",
      "|: 29\n",
      "Description: 2\n",
      "32: 1\n",
      "bytes: 6\n",
      "Hash: 2\n",
      "Pointer: 1\n",
      "4: 2\n",
      "Output: 1\n",
      "Index: 1\n",
      "spent;: 1\n",
      "0: 1\n",
      "1&#x2013;9: 3\n",
      "(VarInt): 3\n",
      "Unlocking-Script: 3\n",
      "Size: 3\n",
      "length: 4\n",
      "bytes,: 4\n",
      "follow: 3\n",
      "Variable: 3\n",
      "fulfills: 1\n",
      "Sequence: 1\n",
      "Number: 1\n",
      "Used: 1\n",
      "locktime: 1\n",
      "disabled: 1\n",
      "(0xFFFFFFFF): 1\n",
      "outputs,: 1\n",
      "let's: 2\n",
      "if: 2\n",
      "format.: 1\n",
      "First,: 1\n",
      "decoded:: 1\n",
      "],: 1\n",
      "Now,: 1\n",
      "these: 1\n",
      "fields: 1\n",
      "hex: 2\n",
      "encoding: 1\n",
      "<<example_6_2>>:: 1\n",
      "[[example_6_2]]: 1\n",
      ".Alice's: 1\n",
      "presented: 1\n",
      "hexadecimal: 1\n",
      "notation: 1\n",
      "+0100000001+*+186f9f998a5aa6f048e51dd8419a14d8a0f1a8a2836dd73+*: 1\n",
      "*+4d2804fe65fa35779000000008b483045022100884d142d86652a3f47+*: 1\n",
      "*+ba4746ec719bbfbd040a570b1deccbb6498c75c4ae24cb02204b9f039+*: 1\n",
      "*+ff08df09cbe9f6addac960298cad530a863ea8f53982c09db8f6e3813+*: 1\n",
      "*+01410484ecc0d46f1918b30928fa0e4ed99f16a0fb4fde0735e7ade84+*: 1\n",
      "*+16ab9fe423cc5412336376789d172787ec3457eee41c04f4938de5cc1+*: 1\n",
      "*+7b4a10fa336a8d752adfffffffff+*+0260e31600000000001976a914ab6+: 1\n",
      "+8025513c3dbd2f7b92a94e0581f5d50f654e788acd0ef800000000000+: 1\n",
      "+1976a9147f9b1a7fb68d60c536c2fd8aeaa53a8f3cc025a888ac00000+: 1\n",
      "+000+: 1\n",
      "Hints:: 1\n",
      "ID: 1\n",
      "reversed: 1\n",
      "order,: 1\n",
      "so: 1\n",
      "starts: 1\n",
      "(hex): 1\n",
      "+18+: 1\n",
      "ends: 1\n",
      "+79+: 1\n",
      "4-byte: 1\n",
      "group: 1\n",
      "zeros,: 1\n",
      "easy: 2\n",
      "+scriptSig+: 1\n",
      "139: 1\n",
      "+8b+: 1\n",
      "+FFFFFFFF+,: 1\n",
      "again: 1\n",
      "identify(((\"\",: 1\n",
      "startref=\"alicesix\"))): 1\n",
      "ScriptSig: 1\n",
      "type: 1\n",
      "<<scriptsig_in_structure>>.: 1\n",
      "field: 2\n",
      "detailed: 2\n",
      "<<seralization_of_signatures_der>>.: 1\n",
      "includes: 2\n",
      "Signature: 4\n",
      "Type: 1\n",
      "(SIGHASH),: 1\n",
      "<<sighash_types>>.: 1\n",
      "[[scriptsig_in_structure]]: 1\n",
      ".ScriptSig: 1\n",
      "produced: 1\n",
      "user’s: 1\n",
      "his: 1\n",
      "her: 1\n",
      "private: 1\n",
      "key,: 2\n",
      "SIGHASH: 1\n",
      "Public: 3\n",
      "Key: 2\n",
      "unhashed: 1\n",
      "Παντελεήμων_Παππάς_mscaidl-0030: 1\n"
     ]
    }
   ],
   "source": [
    "#Print the output\n",
    "output = counts.collect()\n",
    "for (word, count) in output:\n",
    "    print(\"%s: %i\" % (word, count))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DDoS dataset \n",
    "https://www.kaggle.com/datasets/devendra416/ddos-datasets?resource=download\n",
    "\n",
    "This public domain dataset contrains log information from servers the undergo cyber attacks. \n",
    "It is a combination of various cyber attack dataset (i.e CSE-CIC-IDS2018-AWS: https://www.unb.ca/cic/datasets/ids-2017.html\n",
    "CICIDS2017: https://www.unb.ca/cic/datasets/ids-2018.html\n",
    "3.CIC DoS dataset(2016) : https://www.unb.ca/cic/datasets/dos-dataset.html)\n",
    "\n",
    "In this particular case, the dataset focuses on Distributed Denial of Service (DDoS) attacks.\n",
    "For our purposes, we will be using the ballanced dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Locating and loading the csv file\n",
    "path = \"archive/ddos_balanced/final_dataset.csv\"\n",
    "df = spark.read.csv(path,header = 'True',inferSchema='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We also cache the data so that we only read it from disk once.\n",
    "df.cache()\n",
    "df.is_cached            # Checks if df is cached"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12794627, 85)\n"
     ]
    }
   ],
   "source": [
    "# Shape of our data\n",
    "print((df.count(), len(df.columns)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are dealing with a massive ammount of data, and yet Spark lets us process this amount in seconds. Incidentally, pandas is not able to handle this amount of data. (I tried)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- Flow ID: string (nullable = true)\n",
      " |-- Src IP: string (nullable = true)\n",
      " |-- Src Port: integer (nullable = true)\n",
      " |-- Dst IP: string (nullable = true)\n",
      " |-- Dst Port: integer (nullable = true)\n",
      " |-- Protocol: integer (nullable = true)\n",
      " |-- Timestamp: string (nullable = true)\n",
      " |-- Flow Duration: integer (nullable = true)\n",
      " |-- Tot Fwd Pkts: integer (nullable = true)\n",
      " |-- Tot Bwd Pkts: integer (nullable = true)\n",
      " |-- TotLen Fwd Pkts: double (nullable = true)\n",
      " |-- TotLen Bwd Pkts: double (nullable = true)\n",
      " |-- Fwd Pkt Len Max: double (nullable = true)\n",
      " |-- Fwd Pkt Len Min: double (nullable = true)\n",
      " |-- Fwd Pkt Len Mean: double (nullable = true)\n",
      " |-- Fwd Pkt Len Std: double (nullable = true)\n",
      " |-- Bwd Pkt Len Max: double (nullable = true)\n",
      " |-- Bwd Pkt Len Min: double (nullable = true)\n",
      " |-- Bwd Pkt Len Mean: double (nullable = true)\n",
      " |-- Bwd Pkt Len Std: double (nullable = true)\n",
      " |-- Flow Byts/s: double (nullable = true)\n",
      " |-- Flow Pkts/s: double (nullable = true)\n",
      " |-- Flow IAT Mean: double (nullable = true)\n",
      " |-- Flow IAT Std: double (nullable = true)\n",
      " |-- Flow IAT Max: double (nullable = true)\n",
      " |-- Flow IAT Min: double (nullable = true)\n",
      " |-- Fwd IAT Tot: double (nullable = true)\n",
      " |-- Fwd IAT Mean: double (nullable = true)\n",
      " |-- Fwd IAT Std: double (nullable = true)\n",
      " |-- Fwd IAT Max: double (nullable = true)\n",
      " |-- Fwd IAT Min: double (nullable = true)\n",
      " |-- Bwd IAT Tot: double (nullable = true)\n",
      " |-- Bwd IAT Mean: double (nullable = true)\n",
      " |-- Bwd IAT Std: double (nullable = true)\n",
      " |-- Bwd IAT Max: double (nullable = true)\n",
      " |-- Bwd IAT Min: double (nullable = true)\n",
      " |-- Fwd PSH Flags: integer (nullable = true)\n",
      " |-- Bwd PSH Flags: integer (nullable = true)\n",
      " |-- Fwd URG Flags: integer (nullable = true)\n",
      " |-- Bwd URG Flags: integer (nullable = true)\n",
      " |-- Fwd Header Len: integer (nullable = true)\n",
      " |-- Bwd Header Len: integer (nullable = true)\n",
      " |-- Fwd Pkts/s: double (nullable = true)\n",
      " |-- Bwd Pkts/s: double (nullable = true)\n",
      " |-- Pkt Len Min: double (nullable = true)\n",
      " |-- Pkt Len Max: double (nullable = true)\n",
      " |-- Pkt Len Mean: double (nullable = true)\n",
      " |-- Pkt Len Std: double (nullable = true)\n",
      " |-- Pkt Len Var: double (nullable = true)\n",
      " |-- FIN Flag Cnt: integer (nullable = true)\n",
      " |-- SYN Flag Cnt: integer (nullable = true)\n",
      " |-- RST Flag Cnt: integer (nullable = true)\n",
      " |-- PSH Flag Cnt: integer (nullable = true)\n",
      " |-- ACK Flag Cnt: integer (nullable = true)\n",
      " |-- URG Flag Cnt: integer (nullable = true)\n",
      " |-- CWE Flag Count: integer (nullable = true)\n",
      " |-- ECE Flag Cnt: integer (nullable = true)\n",
      " |-- Down/Up Ratio: double (nullable = true)\n",
      " |-- Pkt Size Avg: double (nullable = true)\n",
      " |-- Fwd Seg Size Avg: double (nullable = true)\n",
      " |-- Bwd Seg Size Avg: double (nullable = true)\n",
      " |-- Fwd Byts/b Avg: integer (nullable = true)\n",
      " |-- Fwd Pkts/b Avg: integer (nullable = true)\n",
      " |-- Fwd Blk Rate Avg: integer (nullable = true)\n",
      " |-- Bwd Byts/b Avg: integer (nullable = true)\n",
      " |-- Bwd Pkts/b Avg: integer (nullable = true)\n",
      " |-- Bwd Blk Rate Avg: integer (nullable = true)\n",
      " |-- Subflow Fwd Pkts: integer (nullable = true)\n",
      " |-- Subflow Fwd Byts: integer (nullable = true)\n",
      " |-- Subflow Bwd Pkts: integer (nullable = true)\n",
      " |-- Subflow Bwd Byts: integer (nullable = true)\n",
      " |-- Init Fwd Win Byts: integer (nullable = true)\n",
      " |-- Init Bwd Win Byts: integer (nullable = true)\n",
      " |-- Fwd Act Data Pkts: integer (nullable = true)\n",
      " |-- Fwd Seg Size Min: integer (nullable = true)\n",
      " |-- Active Mean: double (nullable = true)\n",
      " |-- Active Std: double (nullable = true)\n",
      " |-- Active Max: double (nullable = true)\n",
      " |-- Active Min: double (nullable = true)\n",
      " |-- Idle Mean: double (nullable = true)\n",
      " |-- Idle Std: double (nullable = true)\n",
      " |-- Idle Max: double (nullable = true)\n",
      " |-- Idle Min: double (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_c0',\n",
       " 'Flow ID',\n",
       " 'Src IP',\n",
       " 'Src Port',\n",
       " 'Dst IP',\n",
       " 'Dst Port',\n",
       " 'Protocol',\n",
       " 'Timestamp',\n",
       " 'Flow Duration',\n",
       " 'Tot Fwd Pkts',\n",
       " 'Tot Bwd Pkts',\n",
       " 'TotLen Fwd Pkts',\n",
       " 'TotLen Bwd Pkts',\n",
       " 'Fwd Pkt Len Max',\n",
       " 'Fwd Pkt Len Min',\n",
       " 'Fwd Pkt Len Mean',\n",
       " 'Fwd Pkt Len Std',\n",
       " 'Bwd Pkt Len Max',\n",
       " 'Bwd Pkt Len Min',\n",
       " 'Bwd Pkt Len Mean',\n",
       " 'Bwd Pkt Len Std',\n",
       " 'Flow Byts/s',\n",
       " 'Flow Pkts/s',\n",
       " 'Flow IAT Mean',\n",
       " 'Flow IAT Std',\n",
       " 'Flow IAT Max',\n",
       " 'Flow IAT Min',\n",
       " 'Fwd IAT Tot',\n",
       " 'Fwd IAT Mean',\n",
       " 'Fwd IAT Std',\n",
       " 'Fwd IAT Max',\n",
       " 'Fwd IAT Min',\n",
       " 'Bwd IAT Tot',\n",
       " 'Bwd IAT Mean',\n",
       " 'Bwd IAT Std',\n",
       " 'Bwd IAT Max',\n",
       " 'Bwd IAT Min',\n",
       " 'Fwd PSH Flags',\n",
       " 'Bwd PSH Flags',\n",
       " 'Fwd URG Flags',\n",
       " 'Bwd URG Flags',\n",
       " 'Fwd Header Len',\n",
       " 'Bwd Header Len',\n",
       " 'Fwd Pkts/s',\n",
       " 'Bwd Pkts/s',\n",
       " 'Pkt Len Min',\n",
       " 'Pkt Len Max',\n",
       " 'Pkt Len Mean',\n",
       " 'Pkt Len Std',\n",
       " 'Pkt Len Var',\n",
       " 'FIN Flag Cnt',\n",
       " 'SYN Flag Cnt',\n",
       " 'RST Flag Cnt',\n",
       " 'PSH Flag Cnt',\n",
       " 'ACK Flag Cnt',\n",
       " 'URG Flag Cnt',\n",
       " 'CWE Flag Count',\n",
       " 'ECE Flag Cnt',\n",
       " 'Down/Up Ratio',\n",
       " 'Pkt Size Avg',\n",
       " 'Fwd Seg Size Avg',\n",
       " 'Bwd Seg Size Avg',\n",
       " 'Fwd Byts/b Avg',\n",
       " 'Fwd Pkts/b Avg',\n",
       " 'Fwd Blk Rate Avg',\n",
       " 'Bwd Byts/b Avg',\n",
       " 'Bwd Pkts/b Avg',\n",
       " 'Bwd Blk Rate Avg',\n",
       " 'Subflow Fwd Pkts',\n",
       " 'Subflow Fwd Byts',\n",
       " 'Subflow Bwd Pkts',\n",
       " 'Subflow Bwd Byts',\n",
       " 'Init Fwd Win Byts',\n",
       " 'Init Bwd Win Byts',\n",
       " 'Fwd Act Data Pkts',\n",
       " 'Fwd Seg Size Min',\n",
       " 'Active Mean',\n",
       " 'Active Std',\n",
       " 'Active Max',\n",
       " 'Active Min',\n",
       " 'Idle Mean',\n",
       " 'Idle Std',\n",
       " 'Idle Max',\n",
       " 'Idle Min',\n",
       " 'Label']"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The entirety of the datadet can't be properly displayed as it is too large. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "It is hard to distinguish high benign traffic from an actual DDoS attack.\n",
    "\n",
    "Therefore, we will try to find some tell tale signs that can indicate a DDoS attack.\n",
    "The most important features that need to be examined are the following.\n",
    "\n",
    "* Destination Port\n",
    "* Protocol\n",
    "* Flow Duration\n",
    "* Tot Fwd Pkts (Total forward packets)\n",
    "* Tot Bwd Pkts (Total backward packets)\n",
    "* Label (Label)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* General summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+-----------------+--------------------+------------------+------------------+--------+\n",
      "|summary|          Dst Port|         Protocol|       Flow Duration|      Tot Fwd Pkts|      Tot Bwd Pkts|   Label|\n",
      "+-------+------------------+-----------------+--------------------+------------------+------------------+--------+\n",
      "|  count|          12794627|         12794627|            12794627|          12794627|          12794627|12794627|\n",
      "|   mean| 14642.89653078593|7.828587812681057|   8219593.071095234|27.196363285932446|4.9742809227654705|    null|\n",
      "| stddev|23063.826083140073|4.206167602129172|2.4773266927709173E7|1720.5765273371583| 250.9204417677606|    null|\n",
      "|    min|                 0|                0|                  -1|                 0|                 0|  Benign|\n",
      "|    max|             65535|               17|           120000000|            309628|            291923|    ddos|\n",
      "+-------+------------------+-----------------+--------------------+------------------+------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_cols = ['Dst Port','Protocol', \"Flow Duration\", 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'Label']\n",
    "df.select(num_cols).describe().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dst Port and Protocol columns are garbage since thay are categorical values.\n",
    "\n",
    "However the rest of the columns are very useful."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Examining port frenquency "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+-------+\n",
      "|Label|Dst Port|  count|\n",
      "+-----+--------+-------+\n",
      "| ddos|   38092|    141|\n",
      "| ddos|   38088|    101|\n",
      "| ddos|   37760|    150|\n",
      "| ddos|   37778|    148|\n",
      "| ddos|   37672|    135|\n",
      "| ddos|   38154|    121|\n",
      "| ddos|   37784|    138|\n",
      "| ddos|   38192|    115|\n",
      "| ddos|   37654|    162|\n",
      "| ddos|   37880|    147|\n",
      "| ddos|   37846|    115|\n",
      "| ddos|   38106|    116|\n",
      "| ddos|   37804|    135|\n",
      "| ddos|   37958|    133|\n",
      "| ddos|   37796|    162|\n",
      "| ddos|   38050|    132|\n",
      "| ddos|   52114|    185|\n",
      "| ddos|   38004|    128|\n",
      "| ddos|   38180|    144|\n",
      "| ddos|      80|3681958|\n",
      "+-----+--------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Label', 'Dst Port').count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason it seems that port 80 is used a lot more frequently during attacks"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Looking for IP traffic. \n",
    "\n",
    "It seems that ddos attacks use the same IPs over and over again for the attack. Of course, we expected that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+\n",
      "| Label|count(Src IP)|\n",
      "+------+-------------+\n",
      "|  ddos|           36|\n",
      "|Benign|        36726|\n",
      "+------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('Label').agg(countDistinct('Src IP')).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* Flow Duration\n",
    "\n",
    "One of the biggest hints of of DDoS attack is the flow duration of the traffic. So we are looking for the difference in flow duration between normal users and an attacker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+--------------------+---+---------+\n",
      "| Label|  count|                mean|                 std|min|      max|\n",
      "+------+-------+--------------------+--------------------+---+---------+\n",
      "|Benign|6321980|1.3440838487992212E7|3.3109731615055606E7| -1|120000000|\n",
      "|  ddos|6472647|  3119885.1616859375|   9474900.206741018|  0|119999998|\n",
      "+------+-------+--------------------+--------------------+---+---------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Label: string, count: bigint, mean: double, std: double, min: int, max: int]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupby_apply_describe(df, groupby_col='Label', stat_col='Flow Duration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+\n",
      "|summary|       Flow Duration|\n",
      "+-------+--------------------+\n",
      "|  count|            12794627|\n",
      "|   mean|   8219593.071095234|\n",
      "| stddev|2.4773266927709173E7|\n",
      "|    min|                  -1|\n",
      "|    max|           120000000|\n",
      "+-------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe(['Flow Duration']).show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Protocol used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+-------+\n",
      "| Label|Protocol|  count|\n",
      "+------+--------+-------+\n",
      "|  ddos|      17|   4125|\n",
      "|  ddos|       6|6468522|\n",
      "|Benign|       6|4020622|\n",
      "|Benign|      17|2185816|\n",
      "|Benign|       0| 115542|\n",
      "+------+--------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df.groupBy('Label').count('Protocol').collect()\n",
    "df.groupBy(\"Label\",\"Protocol\").count().show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Total Forward Packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----------------+------------------+---+------+\n",
      "| Label|  count|             mean|               std|min|   max|\n",
      "+------+-------+-----------------+------------------+---+------+\n",
      "|Benign|6321980|6.263011904498274|249.79944810340533|  0|219758|\n",
      "|  ddos|6472647|47.64243871170481|2406.2551835248937|  0|309628|\n",
      "+------+-------+-----------------+------------------+---+------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Label: string, count: bigint, mean: double, std: double, min: int, max: int]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupby_apply_describe(df, groupby_col='Label', stat_col='Tot Fwd Pkts')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Backward Packets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+-----------------+-----------------+---+------+\n",
      "| Label|  count|             mean|              std|min|   max|\n",
      "+------+-------+-----------------+-----------------+---+------+\n",
      "|Benign|6321980|7.431544547752445|355.2472252712231|  0|291923|\n",
      "|  ddos|6472647|2.574216236417651|34.37956924413345|  0| 29695|\n",
      "+------+-------+-----------------+-----------------+---+------+\n",
      "\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[Label: string, count: bigint, mean: double, std: double, min: int, max: int]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "groupby_apply_describe(df, groupby_col='Label', stat_col='Tot Bwd Pkts')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using SparkMLlib to train a ML model for DDoS detection"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping relevant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+-------------+------------+------------+-----+\n",
      "|Dst Port|Protocol|Flow Duration|Tot Fwd Pkts|Tot Bwd Pkts|Label|\n",
      "+--------+--------+-------------+------------+------------+-----+\n",
      "|      80|       6|      3974862|          29|          44| ddos|\n",
      "|      80|       6|           63|           1|           1| ddos|\n",
      "|      80|       6|       476078|           2|           6| ddos|\n",
      "|      80|       6|          151|           2|           1| ddos|\n",
      "|      80|       6|       472507|           2|           5| ddos|\n",
      "+--------+--------+-------------+------------+------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = df.select('Dst Port','Protocol', \"Flow Duration\", 'Tot Fwd Pkts', 'Tot Bwd Pkts', 'Label')\n",
    "data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Dst Port: integer (nullable = true)\n",
      " |-- Protocol: integer (nullable = true)\n",
      " |-- Flow Duration: integer (nullable = true)\n",
      " |-- Tot Fwd Pkts: integer (nullable = true)\n",
      " |-- Tot Bwd Pkts: integer (nullable = true)\n",
      " |-- Label: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.printSchema()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing data into vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|[80.0,6.0,3974862...| ddos|\n",
      "|[80.0,6.0,63.0,1....| ddos|\n",
      "|[80.0,6.0,476078....| ddos|\n",
      "+--------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features = data.columns[:-1]\n",
    "\n",
    "va = VectorAssembler(inputCols = features, outputCol='features')\n",
    "\n",
    "va_df = va.transform(data)\n",
    "va_df = va_df.select(['features', 'label'])\n",
    "va_df.show(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(va_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Automatically identify categorical features, and index them.\n",
    "# We specify maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(va_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (30% held out for testing)\n",
    "(trainingData, testData) = va_df.randomSplit([0.7, 0.3])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision Tree model instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a DecisionTree model.\n",
    "dt = DecisionTreeClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chain indexers and tree in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, dt])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(trainingData)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions.\n",
    "predictions = model.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------------+\n",
      "|prediction|indexedLabel|            features|\n",
      "+----------+------------+--------------------+\n",
      "|       0.0|         0.0|[80.0,6.0,0.0,0.0...|\n",
      "|       0.0|         0.0|[80.0,6.0,0.0,0.0...|\n",
      "|       0.0|         0.0|[80.0,6.0,0.0,0.0...|\n",
      "|       0.0|         0.0|[80.0,6.0,0.0,0.0...|\n",
      "|       0.0|         0.0|[80.0,6.0,0.0,0.0...|\n",
      "+----------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Select example rows to display.\n",
    "predictions.select(\"prediction\", \"indexedLabel\", \"features\").show(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 0.951356\n",
      "Test Error = 0.0486441\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Accuracy = %g\" % (accuracy))\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassificationModel: uid=DecisionTreeClassifier_e7ab9ce24721, depth=5, numNodes=39, numClasses=2, numFeatures=5\n"
     ]
    }
   ],
   "source": [
    "treeModel = model.stages[2]\n",
    "print(treeModel) # summary only"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We achieve a 95.14% accuracy with our machine learning model using Spark.\n",
    "\n",
    "Spark was able to handle 63,973,135 (12794627 *5, not including labels) datapoints effortlessly during preprocessing and training. In addition, Spark could be configured to monitor server log entries online. So theoretically, this could be a (relatively simple) first defence against DDoS attacks for a server.\n",
    "\n",
    "Thank you for your attention.\n",
    "\n",
    "Pantelis Pappas.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a712c55ccf82e08841b6807c4c3edca3263ca1d7a92e068e481b2ad97bdf8e29"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
